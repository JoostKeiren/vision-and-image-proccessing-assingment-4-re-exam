{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy.spatial.distance import euclidean, cosine\n",
    "from scipy.stats import entropy\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of categories: 22\n",
      "Category: lamp, Number of images: 61\n",
      "Category: bass, Number of images: 54\n",
      "Category: buddha, Number of images: 85\n",
      "Category: brain, Number of images: 98\n",
      "Category: umbrella, Number of images: 75\n",
      "Category: butterfly, Number of images: 91\n",
      "Category: bonsai, Number of images: 128\n",
      "Category: Faces, Number of images: 435\n",
      "Category: gramophone, Number of images: 51\n",
      "Category: stop_sign, Number of images: 64\n",
      "Category: cellphone, Number of images: 59\n",
      "Category: dollar_bill, Number of images: 52\n",
      "Category: minaret, Number of images: 76\n",
      "Category: helicopter, Number of images: 49\n",
      "Category: ant, Number of images: 42\n",
      "Category: laptop, Number of images: 81\n",
      "Category: cougar_face, Number of images: 69\n",
      "Category: water_lilly, Number of images: 37\n",
      "Category: revolver, Number of images: 82\n",
      "Category: pizza, Number of images: 53\n",
      "Category: camera, Number of images: 50\n",
      "Category: garfield, Number of images: 34\n"
     ]
    }
   ],
   "source": [
    "img_paths = glob('Images/*/*.jpg')\n",
    "labels = [Path(path).parent.name for path in img_paths]\n",
    "\n",
    "label_counts = Counter(labels)\n",
    "\n",
    "print(f'Number of categories: {len(label_counts)}')\n",
    "\n",
    "for label, count in label_counts.items():\n",
    "    print(f'Category: {label}, Number of images: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1826, 1826)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = []\n",
    "for img_path in img_paths:\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    images.append(img)\n",
    "\n",
    "len(images), len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK: We recommend that you (for a start) select a subset of say 4-5 categories. When you have checked that everything works you may extend to say 20 categories. For each category, the\n",
    "set of images should be split in two: A training set and a test set (of equal size). The test set must not include images in the training set. When using few categories you may also limit the number of training images (to say 10) per category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first select 5 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images: 50, Test images: 50\n"
     ]
    }
   ],
   "source": [
    "select_5cat = ['Faces', 'bonsai', 'brain', 'butterfly', 'buddha']\n",
    "\n",
    "train_imgs_5cat = []\n",
    "test_imgs_5cat = []\n",
    "\n",
    "num_images_per_category = 10 ## Limit the number of images per category to 10 for training and 10 for testing\n",
    "\n",
    "for category in select_5cat:\n",
    "    category_imgs = [img_path for img_path, label in zip(img_paths, labels) if label == category]\n",
    "    random.shuffle(category_imgs)\n",
    "    \n",
    "    # Split the images into training and test sets\n",
    "    train_imgs_5cat.extend(category_imgs[:num_images_per_category])\n",
    "    test_imgs_5cat.extend(category_imgs[num_images_per_category:num_images_per_category*2])\n",
    "\n",
    "print(f'Training images: {len(train_imgs_5cat)}, Test images: {len(test_imgs_5cat)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extend to 20 categories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images: 872, Test images: 872\n"
     ]
    }
   ],
   "source": [
    "# Select 20 categories\n",
    "select_20cat = ['Faces', 'bonsai', 'brain', 'butterfly', 'buddha', 'revolver', 'laptop', 'minaret', 'umbrella', 'cougar_face', 'stop_sign', 'lamp', 'cellphone', 'bass', 'pizza', 'dollar_bill', 'gramophone', 'camera', 'helicopter', 'ant']\n",
    "\n",
    "train_imgs_20cat = []\n",
    "test_imgs_20cat = []\n",
    "\n",
    "for category in select_20cat:\n",
    "    category_imgs = [img_path for img_path, label in zip(img_paths, labels) if label == category]\n",
    "    random.shuffle(category_imgs)\n",
    "    \n",
    "    num_images_per_category = len(category_imgs) // 2  # Split the images into training and test sets equally\n",
    "    \n",
    "    train_imgs_20cat.extend(category_imgs[:num_images_per_category])\n",
    "    test_imgs_20cat.extend(category_imgs[num_images_per_category:num_images_per_category*2])\n",
    "\n",
    "print(f'Training images: {len(train_imgs_20cat)}, Test images: {len(test_imgs_20cat)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference: {'garfield', 'water_lilly'}\n"
     ]
    }
   ],
   "source": [
    "labels = list(set(labels))\n",
    "diff = set(labels) - set(select_20cat)\n",
    "print(f'Difference: {diff}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK: You should extract visual words using SIFT descriptors (ignoring position, orientation and scale) or similar descriptors extracted at interest points. To compute the descriptors, we recommend to use OpenCV’s sift, but other options are possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE TO SELF(can delete) : SIFT (Scale-Invariant Feature Transform) - detect and describe local features in images. \n",
    "\n",
    "Scale-space Extrema Detection: Create a series of images by applying Gaussian blur with increasing σ. Subtract each adjacent pair of images to get the DoG images. Identify local maxima and minima in the DoG images by comparing each pixel to its neighbors in both the current image and adjacent scales.\n",
    "\n",
    "Keypoint Localization: Fit a quadratic function to the local sample points to determine the interpolated location of the extremum. Discard keypoints with low contrast (below a threshold) or those that are poorly localized along edges (using the Hessian matrix).\n",
    "\n",
    "Orientation Assignment: For each keypoint, consider a region around it. Compute the gradient magnitude and orientation for each pixel in the region. Create a histogram of gradient orientations (typically 36 bins covering 360 degrees).\n",
    "Assign the dominant orientation(s) to the keypoint. If there are multiple peaks in the histogram, create multiple keypoints with different orientations.\n",
    "\n",
    "Keypoint Descriptor: Take a 16x16 window around the keypoint, oriented according to the keypoint's orientation.\n",
    "Divide this window into 4x4 sub-regions. For each sub-region, compute a histogram of gradient orientations (typically 8 bins).\n",
    "Concatenate these histograms to form a 128-dimensional vector (4x4x8).\n",
    "\n",
    "Matching: For each keypoint descriptor in one image, find the closest descriptor in the other image using a distance metric \n",
    "Apply a ratio test to filter out poor matches (e.g., the ratio of the distance to the closest neighbor to the distance of the second closest neighbor should be below a threshold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of training descriptors for the training set with 20 categories: 429193\n"
     ]
    }
   ],
   "source": [
    "sift = cv2.SIFT_create()\n",
    "\n",
    "def compute_sift_descriptors(image):\n",
    "    keypoints, descriptors = sift.detectAndCompute(image, None)\n",
    "    return descriptors\n",
    "\n",
    "\n",
    "train_descriptors = []\n",
    "for img_path in train_imgs_20cat: #we take the train set with 20 categories\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    descriptors = compute_sift_descriptors(img)\n",
    "    if descriptors is not None:\n",
    "        train_descriptors.append(descriptors)\n",
    "\n",
    "# Concatenate the descriptors into a matrix, one descriptor per row\n",
    "train_descriptors_matrix = np.vstack(train_descriptors)\n",
    "\n",
    "\n",
    "print(f'Total number of training descriptors for the training set with 20 categories: {train_descriptors_matrix.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of training descriptors for the training set with 5 categories: 32937\n"
     ]
    }
   ],
   "source": [
    "train_descriptors_small = []\n",
    "for img_path in train_imgs_5cat: #we take the train set with 5 categories\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    descriptors = compute_sift_descriptors(img)\n",
    "    if descriptors is not None:\n",
    "        train_descriptors_small.append(descriptors)\n",
    "\n",
    "train_descriptors_matrix_small = np.vstack(train_descriptors_small)\n",
    "\n",
    "print(f'Total number of training descriptors for the training set with 5 categories: {train_descriptors_matrix_small.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out one of the descriptors\n",
    "print(train_descriptors_matrix[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Codebook generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: In order to generate a code book, select a set of training images. Then Extract\n",
    "SIFT features from the training images (ignore position, orientation and scale).\n",
    "The SIFT features should be concatenated into a matrix, one descriptor per row.\n",
    "Then you should run the k-means clustering algorithm on the subset of training\n",
    "descriptors to extract good prototype (visual word) clusters. A reasonable k\n",
    "should be small (say between 200 and 500) for a small number of categories\n",
    "(say 5) and larger (say between 500 and 2000) for a larger number of categories.\n",
    "Also, a good value of k may depend on the complexity of your data. You should\n",
    "experiment with a few different values of k (but beware that this can be rather\n",
    "time-consuming).\n",
    "\n",
    "Once clustering has been obtained, classify each training descriptor to the\n",
    "closest cluster centers) and form the bag of words (BoW) for each image in the\n",
    "image training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are first evaluating which is the best value for k for the subset with 5 categories. We report the Silhouette score.\n",
    "The Silhouette score is a metric used to evaluate the quality of clustering. It measures how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The score ranges from -1 to 1. 1: Indicates that the sample is far away from the neighboring clusters and well matched to its own cluster. 0: Indicates that the sample is on or very close to the decision boundary between two neighboring clusters. -1: Indicates that the sample might have been assigned to the wrong cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=200, Silhouette Score=0.03397717326879501\n",
      "k=300, Silhouette Score=0.03205038979649544\n",
      "k=400, Silhouette Score=0.030650857836008072\n",
      "k=500, Silhouette Score=0.03012615442276001\n",
      "Best k for 5 categories: 200\n"
     ]
    }
   ],
   "source": [
    "def evaluate_k_means(descriptors, k_values):\n",
    "    best_k = None\n",
    "    best_score = -1\n",
    "    for k in k_values:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=0).fit(descriptors)\n",
    "        labels = kmeans.labels_\n",
    "        score = silhouette_score(descriptors, labels)\n",
    "        print(f'k={k}, Silhouette Score={score}')\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_k = k\n",
    "    return best_k\n",
    "\n",
    "k_values_5cat = range(200, 501, 100)\n",
    "best_k_5cat = evaluate_k_means(train_descriptors_matrix_small, k_values_5cat)\n",
    "print(f'Best k for 5 categories: {best_k_5cat}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO:\n",
    "#from sklearn.cluster import MiniBatchKMeans\n",
    "#from joblib import Parallel, delayed\n",
    "#\n",
    "#def evaluate_k_means_minibatch(descriptors, k_values):\n",
    "#    best_k = None\n",
    "#    best_score = -1\n",
    "#\n",
    "#    def evaluate_single_k(k):\n",
    "#        kmeans = MiniBatchKMeans(n_clusters=k, random_state=0, batch_size=500).fit(descriptors)\n",
    "#        labels = kmeans.labels_\n",
    "#        score = silhouette_score(descriptors, labels)\n",
    "#        return k, score\n",
    "#\n",
    "#    results = Parallel(n_jobs=-1)(delayed(evaluate_single_k)(k) for k in k_values)\n",
    "#\n",
    "#    for k, score in results:\n",
    "#        print(f'k={k}, Silhouette Score={score}')\n",
    "#        if score > best_score:\n",
    "#            best_score = score\n",
    "#            best_k = k\n",
    "#\n",
    "#    return best_k\n",
    "#\n",
    "#k_values_20cat = range(500, 2000, 500) \n",
    "#best_k_20cat = evaluate_k_means_minibatch(train_descriptors_matrix, k_values_20cat)\n",
    "#print(f'Best k for the training set with 20 categories: {best_k_20cat}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we implement the k-means clustering on the training descriptors for the 2 different subsets. Clustering results to classify each descriptor to the closest cluster center. Then, for each image, count the number of descriptors assigned to each cluster to form a histogram (BoW vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of visual words (clusters): 200\n"
     ]
    }
   ],
   "source": [
    "kmeans_5 = KMeans(n_clusters=best_k_5cat, random_state=42)\n",
    "kmeans_5.fit(train_descriptors_matrix_small)\n",
    "\n",
    "visual_words = kmeans_5.cluster_centers_ # Get the cluster centers (visual words)\n",
    "\n",
    "print(f'Number of visual words (clusters): {visual_words.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: \n",
    "#kmeans_20 = KMeans(n_clusters=best_k_20cat, random_state=42)\n",
    "#kmeans_20.fit(train_descriptors_matrix)\n",
    "#\n",
    "#visual_words_20 = kmeans_20.cluster_centers_\n",
    "#\n",
    "#print(f'Number of visual words (clusters): {visual_words_20.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BoW representations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of BoW vectors for the training set with 5 categories: 50\n"
     ]
    }
   ],
   "source": [
    "def form_bow(descriptors_list, kmeans):\n",
    "    bow_list = []\n",
    "    for descriptors in descriptors_list:\n",
    "        labels = kmeans.predict(descriptors) # # Predict the closest cluster for each descriptor\n",
    "        bow = np.bincount(labels, minlength=kmeans.n_clusters) # Create a histogram of cluster assignments\n",
    "        bow_list.append(bow)\n",
    "    return np.array(bow_list)\n",
    "\n",
    "bow_list_5cat = form_bow(train_descriptors_small, kmeans_5)\n",
    "\n",
    "print(f'Number of BoW vectors for the training set with 5 categories: {len(bow_list_5cat)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO:\n",
    "#bow_list_20cat = form_bow(train_descriptors, kmeans_20)\n",
    "#print(f'Number of BoW vectors for the training set with 20 categories: {len(bow_list_20cat)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK: The next step consists in content indexing. For each image in the test set you\n",
    "should:\n",
    "\n",
    "• Extract the SIFT descriptors of the feature points in the image,\n",
    "\n",
    "• Project the descriptors onto the codebook, i.e., for each descriptor the\n",
    "closest cluster prototype should be found,\n",
    "\n",
    "• Construct the generated corresponding bag of words, i.e, word histogram.\n",
    "Please note that you have already performed the same steps for the training\n",
    "images during codebook generation.\n",
    "\n",
    "Now construct and save a table that would contain, per entry at least the\n",
    "file name, the true category, if it belongs to the training- or test set, and the\n",
    "corresponding bag of words / word histogram. The table need only be computed\n",
    "once and then used repeatably in the following retrieval experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of test descriptors for the test set with 5 categories: 24941\n"
     ]
    }
   ],
   "source": [
    "test_descriptors_small = []\n",
    "for img_path in test_imgs_5cat:\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    descriptors = compute_sift_descriptors(img)\n",
    "    if descriptors is not None:\n",
    "        test_descriptors_small.append(descriptors)\n",
    "\n",
    "test_descriptors_matrix_small = np.vstack(test_descriptors_small)\n",
    "\n",
    "print(f'Total number of test descriptors for the test set with 5 categories: {test_descriptors_matrix_small.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of BoW vectors for the test set with 5 categories: 50\n"
     ]
    }
   ],
   "source": [
    "bow_list_test_5cat = form_bow(test_descriptors_small, kmeans_5)\n",
    "\n",
    "print(f'Number of BoW vectors for the test set with 5 categories: {len(bow_list_test_5cat)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO\n",
    "#test_descriptors = []\n",
    "#for img_path in test_imgs_20cat:\n",
    "#    img = cv2.imread(img_path)\n",
    "#    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#    descriptors = compute_sift_descriptors(img)\n",
    "#    if descriptors is not None:\n",
    "#        test_descriptors.append(descriptors)\n",
    "#\n",
    "#test_descriptors_matrix = np.vstack(test_descriptors)\n",
    "#\n",
    "#print(f'Total number of test descriptors for the test set with 20 categories: {test_descriptors_matrix.shape[0]}')\n",
    "\n",
    "#bow_list_test_20cat = form_bow(test_descriptors, kmeans_20)\n",
    "\n",
    "#print(f'Number of BoW vectors for the test set with 20 categories: {len(bow_list_test_20cat)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table saved as 'bow_table_5cat.csv'\n"
     ]
    }
   ],
   "source": [
    "data_5cat = []\n",
    "\n",
    "for img_path, bow in zip(train_imgs_5cat, bow_list_5cat):\n",
    "    category = Path(img_path).parent.name\n",
    "    data_5cat.append([img_path, category, 'train', bow])\n",
    "\n",
    "for img_path, bow in zip(test_imgs_5cat, bow_list_test_5cat):\n",
    "    category = Path(img_path).parent.name\n",
    "    data_5cat.append([img_path, category, 'test', bow])\n",
    "\n",
    "df_bow_5cat = pd.DataFrame(data_5cat, columns=['file_name', 'category', 'set_type', 'bow'])\n",
    "\n",
    "df_bow_5cat.to_csv('bow_table_5cat.csv', index=False)\n",
    "\n",
    "print(\"Table saved as 'bow_table_5cat.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>category</th>\n",
       "      <th>set_type</th>\n",
       "      <th>bow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Images/Faces/image_0408.jpg</td>\n",
       "      <td>Faces</td>\n",
       "      <td>train</td>\n",
       "      <td>[4, 3, 2, 5, 2, 7, 4, 0, 4, 8, 1, 1, 0, 0, 19,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Images/Faces/image_0239.jpg</td>\n",
       "      <td>Faces</td>\n",
       "      <td>train</td>\n",
       "      <td>[4, 1, 1, 20, 1, 2, 1, 2, 2, 8, 1, 1, 5, 2, 18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Images/Faces/image_0148.jpg</td>\n",
       "      <td>Faces</td>\n",
       "      <td>train</td>\n",
       "      <td>[3, 7, 2, 15, 1, 4, 8, 2, 2, 6, 0, 0, 1, 0, 3,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Images/Faces/image_0036.jpg</td>\n",
       "      <td>Faces</td>\n",
       "      <td>train</td>\n",
       "      <td>[2, 1, 1, 6, 2, 5, 5, 1, 4, 0, 2, 2, 0, 0, 22,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Images/Faces/image_0402.jpg</td>\n",
       "      <td>Faces</td>\n",
       "      <td>train</td>\n",
       "      <td>[1, 1, 4, 7, 2, 7, 1, 5, 1, 1, 2, 3, 1, 0, 3, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     file_name category set_type  \\\n",
       "0  Images/Faces/image_0408.jpg    Faces    train   \n",
       "1  Images/Faces/image_0239.jpg    Faces    train   \n",
       "2  Images/Faces/image_0148.jpg    Faces    train   \n",
       "3  Images/Faces/image_0036.jpg    Faces    train   \n",
       "4  Images/Faces/image_0402.jpg    Faces    train   \n",
       "\n",
       "                                                 bow  \n",
       "0  [4, 3, 2, 5, 2, 7, 4, 0, 4, 8, 1, 1, 0, 0, 19,...  \n",
       "1  [4, 1, 1, 20, 1, 2, 1, 2, 2, 8, 1, 1, 5, 2, 18...  \n",
       "2  [3, 7, 2, 15, 1, 4, 8, 2, 2, 6, 0, 0, 1, 0, 3,...  \n",
       "3  [2, 1, 1, 6, 2, 5, 5, 1, 4, 0, 2, 2, 0, 0, 22,...  \n",
       "4  [1, 1, 4, 7, 2, 7, 1, 5, 1, 1, 2, 3, 1, 0, 3, ...  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bow_5cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO\n",
    "#data_20cat = []\n",
    "#\n",
    "#for img_path, bow in zip(train_imgs_20cat, bow_list_20cat):\n",
    "#    category = Path(img_path).parent.name\n",
    "#    data_20cat.append([img_path, category, 'train', bow])\n",
    "#\n",
    "#for img_path, bow in zip(test_imgs_20cat, bow_list_test_20cat):\n",
    "#    category = Path(img_path).parent.name\n",
    "#    data_20cat.append([img_path, category, 'test', bow])\n",
    "#\n",
    "#df_bow_20cat = pd.DataFrame(data_20cat, columns=['file_name', 'category', 'set_type', 'bow'])\n",
    "#\n",
    "#df_bow_20cat.to_csv('bow_table_20cat.csv', index=False)\n",
    "#\n",
    "#print(\"Table saved as 'bow_table_20cat.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Retrieving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK: Finally, you should implement retrieving of images using some of the similarity\n",
    "measures discussed in the course slides. You may use: \n",
    "\n",
    "• common words\n",
    "\n",
    "• tf-ifd similarity\n",
    "\n",
    "• Bhattacharyya distance or Kullback-Leibler divergence\n",
    "\n",
    "Please argue for your choice or report the differences in result when applying\n",
    "the different measures.\n",
    "\n",
    "Your report should show commented results for two experiments. \n",
    "\n",
    "In the first\n",
    "you consider retrieving training images. \n",
    "\n",
    "In the second you test how well you\n",
    "can classify test images. Otherwise the two test are identical. \n",
    "\n",
    "For each test you\n",
    "should count:\n",
    "• The mean reciprocal rank (i.e. the average across all queries of 1/ranki ,\n",
    "where ranki is the rank position of the first correct category for the i’th\n",
    "query).\n",
    "• How often (in per cent) the correct category is in top-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 category subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('bow_table_5cat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_bow(bow_string):\n",
    "    \"\"\"\n",
    "    Parses a bag-of-words (BOW) string representation into a NumPy array.\n",
    "\n",
    "    Parameters:\n",
    "    - bow_string: str, the BOW string representation with numbers separated by spaces.\n",
    "\n",
    "    Returns:\n",
    "    - NumPy array of integers representing the BOW vector.\n",
    "    \"\"\"\n",
    "    clean_bow = bow_string.replace('\\n', ' ').replace('  ', ' ').strip('[]')\n",
    "    bow_list = [int(x) for x in clean_bow.split()]\n",
    "    return np.array(bow_list)\n",
    "\n",
    "\n",
    "def common_words_similarity(bow1, bow2):\n",
    "    \"\"\"\n",
    "    Computes the similarity between two BOW vectors based on the number of common words.\n",
    "\n",
    "    Parameters:\n",
    "    - bow1: NumPy array, the first BOW vector.\n",
    "    - bow2: NumPy array, the second BOW vector.\n",
    "\n",
    "    Returns:\n",
    "    - Integer representing the sum of the minimum word counts for each term in the vectors.\n",
    "    \"\"\"\n",
    "    return np.sum(np.minimum(bow1, bow2))\n",
    "\n",
    "\n",
    "def compute_idf(df):\n",
    "    \"\"\"\n",
    "    Computes the Inverse Document Frequency (IDF) for terms in the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - df: Pandas DataFrame, containing the 'bow' column with parsed BOW vectors.\n",
    "\n",
    "    Returns:\n",
    "    - NumPy array of IDF values for each term in the vocabulary.\n",
    "    \"\"\"\n",
    "    num_docs = len(df)\n",
    "    term_frequencies = np.sum(np.vstack(df['bow']), axis=0)  # Sum BOW vectors across all documents\n",
    "    idf = np.log((num_docs + 1) / (1 + term_frequencies)) + 1  # Smooth IDF\n",
    "    return idf\n",
    "\n",
    "\n",
    "def tf_idf_similarity(bow1, bow2, idf):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity between two TF-IDF vectors.\n",
    "\n",
    "    Parameters:\n",
    "    - bow1: NumPy array, the first BOW vector.\n",
    "    - bow2: NumPy array, the second BOW vector.\n",
    "    - idf: NumPy array, precomputed IDF values.\n",
    "\n",
    "    Returns:\n",
    "    - Float representing the cosine similarity between the two TF-IDF vectors.\n",
    "    \"\"\"\n",
    "    tf_idf1 = bow1 * idf\n",
    "    tf_idf2 = bow2 * idf\n",
    "    return 1 - cosine(tf_idf1, tf_idf2)\n",
    "\n",
    "\n",
    "def bhattacharyya_distance(bow1, bow2):\n",
    "    \"\"\"\n",
    "    Computes the Bhattacharyya distance between two probability distributions.\n",
    "\n",
    "    Parameters:\n",
    "    - bow1: NumPy array, the first BOW vector.\n",
    "    - bow2: NumPy array, the second BOW vector.\n",
    "\n",
    "    Returns:\n",
    "    - Float representing the Bhattacharyya distance.\n",
    "    \"\"\"\n",
    "    return -np.log(np.sum(np.sqrt(bow1 * bow2)))\n",
    "\n",
    "\n",
    "def kl_divergence(bow1, bow2):\n",
    "    \"\"\"\n",
    "    Computes the Kullback-Leibler (KL) divergence between two probability distributions.\n",
    "\n",
    "    Parameters:\n",
    "    - bow1: NumPy array, the first probability distribution.\n",
    "    - bow2: NumPy array, the second probability distribution.\n",
    "\n",
    "    Returns:\n",
    "    - Float representing the KL divergence.\n",
    "    \"\"\"\n",
    "    return entropy(bow1, bow2)\n",
    "\n",
    "\n",
    "def retrieve_images(query_bow, df, similarity_measure):\n",
    "    \"\"\"\n",
    "    Retrieves images from a dataset based on similarity to a query BOW vector.\n",
    "\n",
    "    Parameters:\n",
    "    - query_bow: NumPy array, the query BOW vector.\n",
    "    - df: Pandas DataFrame, containing images and their BOW vectors.\n",
    "    - similarity_measure: Function, the similarity measure to use for comparison.\n",
    "\n",
    "    Returns:\n",
    "    - List of tuples containing file name, category, and similarity score, sorted by similarity.\n",
    "    \"\"\"\n",
    "    similarities = []\n",
    "    for _, row in df.iterrows():\n",
    "        bow = row['bow']  # Use the parsed NumPy array directly\n",
    "        similarity = similarity_measure(query_bow, bow)\n",
    "        similarities.append((row['file_name'], row['category'], similarity))\n",
    "    similarities.sort(key=lambda x: x[2], reverse=True)\n",
    "    return similarities\n",
    "\n",
    "\n",
    "def evaluate_retrieval(df, similarity_measure):\n",
    "    \"\"\"\n",
    "    Evaluates the retrieval performance of a similarity measure using Mean Reciprocal Rank (MRR) and Top-3 accuracy.\n",
    "\n",
    "    Parameters:\n",
    "    - df: Pandas DataFrame, containing images and their BOW vectors.\n",
    "    - similarity_measure: Function, the similarity measure to use for comparison.\n",
    "\n",
    "    Returns:\n",
    "    - mrr: Float, the Mean Reciprocal Rank across all queries.\n",
    "    - top3_percentage: Float, the percentage of queries where the correct category is in the top 3 results.\n",
    "    \"\"\"\n",
    "    mrr = 0\n",
    "    top3_count = 0\n",
    "    num_queries = len(df)\n",
    "\n",
    "    for query_index in range(num_queries):\n",
    "        query_bow = df.iloc[query_index]['bow']\n",
    "        query_category = df.iloc[query_index]['category']\n",
    "        results = retrieve_images(query_bow, df, similarity_measure)\n",
    "\n",
    "        # Calculate rank of the first correct category\n",
    "        rank = next((i for i, result in enumerate(results) if result[1] == query_category), None)\n",
    "        if rank is not None:\n",
    "            mrr += 1 / (rank + 1)\n",
    "            if rank < 3:\n",
    "                top3_count += 1\n",
    "\n",
    "    mrr /= num_queries\n",
    "    top3_percentage = (top3_count / num_queries) * 100\n",
    "    return mrr, top3_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform experiments and print results\n",
    "def perform_experiment(set_name, dataset, similarity_measures):\n",
    "    print(f\"\\n--- Results for {set_name} Set ---\")\n",
    "    for name, similarity_measure in similarity_measures.items():\n",
    "        mrr, top3 = evaluate_retrieval(dataset, similarity_measure)\n",
    "        print(f\"Similarity Measure: {name}\")\n",
    "        print(f\"  - Mean Reciprocal Rank (MRR): {mrr:.4f}\")\n",
    "        print(f\"  - Top-3 Accuracy: {top3:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bow'] = df['bow'].apply(parse_bow)\n",
    "\n",
    "idf = compute_idf(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1 - retrieving training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_102355/3413021990.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_set['bow'] = training_set['bow'].apply(parse_bow)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Results for Training Set ---\n",
      "Similarity Measure: Common Words\n",
      "  - Mean Reciprocal Rank (MRR): 1.0000\n",
      "  - Top-3 Accuracy: 100.00%\n",
      "Similarity Measure: TF-IDF\n",
      "  - Mean Reciprocal Rank (MRR): 1.0000\n",
      "  - Top-3 Accuracy: 100.00%\n",
      "Similarity Measure: Bhattacharyya\n",
      "  - Mean Reciprocal Rank (MRR): 0.3560\n",
      "  - Top-3 Accuracy: 52.00%\n",
      "Similarity Measure: KL Divergence\n",
      "  - Mean Reciprocal Rank (MRR): 0.2392\n",
      "  - Top-3 Accuracy: 20.00%\n"
     ]
    }
   ],
   "source": [
    "training_set = df[df['set_type'] == 'train']\n",
    "\n",
    "training_set['bow'] = training_set['bow'].apply(parse_bow)\n",
    "\n",
    "similarity_measures = { 'Common Words': common_words_similarity, \n",
    "                       'TF-IDF': lambda bow1, bow2: tf_idf_similarity(bow1, bow2, idf), \n",
    "                       'Bhattacharyya': bhattacharyya_distance, \n",
    "                       'KL Divergence': kl_divergence }\n",
    "\n",
    "perform_experiment(\"Training\", training_set, similarity_measures)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2 - retrieving test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_102355/4224355731.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_set['bow'] = test_set['bow'].apply(parse_bow)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Results for Test Set ---\n",
      "Similarity Measure: Common Words\n",
      "  - Mean Reciprocal Rank (MRR): 1.0000\n",
      "  - Top-3 Accuracy: 100.00%\n",
      "Similarity Measure: TF-IDF\n",
      "  - Mean Reciprocal Rank (MRR): 1.0000\n",
      "  - Top-3 Accuracy: 100.00%\n",
      "Similarity Measure: Bhattacharyya\n",
      "  - Mean Reciprocal Rank (MRR): 0.3556\n",
      "  - Top-3 Accuracy: 32.00%\n",
      "Similarity Measure: KL Divergence\n",
      "  - Mean Reciprocal Rank (MRR): 0.2393\n",
      "  - Top-3 Accuracy: 20.00%\n"
     ]
    }
   ],
   "source": [
    "test_set = df[df['set_type'] == 'test']\n",
    "\n",
    "test_set['bow'] = test_set['bow'].apply(parse_bow)\n",
    "\n",
    "perform_experiment(\"Test\", test_set, similarity_measures)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
